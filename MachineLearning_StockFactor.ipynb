{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import math\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.mlab as mlab\n",
    "\n",
    "from sklearn import metrics\n",
    "import matplotlib.pyplot as plt\n",
    "from pyecharts import Line, Scatter, Bar, Overlap, Timeline, HeatMap, ThemeRiver, Grid, Page\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.naive_bayes import GaussianNB, BernoulliNB, MultinomialNB\n",
    "from lightgbm.sklearn import LGBMClassifier # LightGBM\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from ipywidgets import interact, fixed\n",
    "\n",
    "from scipy.stats import spearmanr\n",
    "import heapq\n",
    "import copy\n",
    "from tqdm import tnrange,tqdm_notebook\n",
    "from time import sleep\n",
    "\n",
    "import scipy.io as sio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "read_path=r'C:\\Users\\DYT\\Desktop\\Data\\data_system\\stock\\daily\\20180425'\n",
    "factor_path = r'C:\\Users\\DYT\\Desktop\\Data\\data_system\\stock\\ml_factor180614new'\n",
    "factor_pathold = r'C:\\Users\\DYT\\Desktop\\Data\\data_system\\stock\\ml_factor'\n",
    "######### read .mat file in \"data\" directory ########\n",
    "def read_data(name,path=read_path):\n",
    "    path = path + '\\\\' + name\n",
    "    return sio.loadmat(path)[name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_st_stock = read_data('valid_st_stock')\n",
    "ipo_date = read_data('ipo_date')\n",
    "stock_rt = read_data('stock_rt', path=factor_path)\n",
    "gfn1_code_index = read_data('gfn1_code_index')\n",
    "trd_dates = read_data('trd_dates')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# obtain 161 factor names as lsit factor_names_old\n",
    "# obtain new 10 factors as ml_zs_names\n",
    "# there are 171 factors in total and the whole list is named as ml_names_all\n",
    "factor_names_old = []\n",
    "col_vec = []\n",
    "for info in os.listdir(factor_pathold):\n",
    "    index = info.rfind('.')\n",
    "    name = info[:index]\n",
    "    factor_names_old.append(name)\n",
    "\n",
    "factor_names_new = []\n",
    "col_vec = []\n",
    "for info in os.listdir(factor_path):\n",
    "    index = info.rfind('.')\n",
    "    name = info[:index]\n",
    "    factor_names_new.append(name)\n",
    "\n",
    "factor_names_new.remove('stock_rt')\n",
    "\n",
    "ml_zs_names = []\n",
    "ml_bk_names = []\n",
    "ml_bk_rt_names = []\n",
    "for info in factor_names_new:\n",
    "    for sufix in ['zs','bk','rt']:\n",
    "        if info[-2:] == 'zs':\n",
    "            ml_zs_names.append(info)\n",
    "        elif info[-2:] == 'bk':\n",
    "            ml_bk_names.append(info)\n",
    "        elif info[-2:] == 'rt':\n",
    "            ml_bk_rt_names.append(info)\n",
    "            \n",
    "factor_new_ml_names = sorted(list(set(factor_names_new).\n",
    "                                  difference(set(ml_zs_names + \n",
    "                                                 ml_bk_names + \n",
    "                                                 ml_bk_rt_names))))\n",
    "ml_zs_names = sorted(np.unique(np.array(ml_zs_names)).tolist())\n",
    "ml_bk_names = sorted(np.unique(np.array(ml_bk_names)).tolist())\n",
    "ml_bk_rt_names = sorted(np.unique(np.array(ml_bk_rt_names)).tolist())\n",
    "\n",
    "factor_names_old = sorted(factor_names_old)\n",
    "ml_zs_names = sorted(ml_zs_names)\n",
    "\n",
    "factor_names_old.remove('falpha5')\n",
    "factor_names_old.remove('falpha36')\n",
    "factor_names_old.remove('falpha51')\n",
    "factor_names_old.remove('falpha11')\n",
    "ml_names_all = sorted(factor_names_old + ml_zs_names)\n",
    "\n",
    "\n",
    "for info in factor_names_old:\n",
    "    globals()[info] = read_data(info, path=factor_pathold)\n",
    "    \n",
    "for info in ml_zs_names:\n",
    "    globals()[info] = read_data(info, path=factor_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modmat = np.array([[0, 0, 0, 0, 0, 1, 1, 1, 1, 1],\n",
    "                   [0, 0, 0, 1, 1, 0, 0, 0, 1, 1],\n",
    "                   [0, 0, 1, 0, 0, 0, 1, 1, 0, 1],\n",
    "                   [0, 0, 0, 0, 1, 1, 0, 1, 1, 1],\n",
    "                   [0, 0, 0, 0, 0, 0, 1, 0, 1, 0],\n",
    "                   [0, 1, 0, 1, 0, 1, 1, 1, 1, 1],\n",
    "                   [0, 0, 0, 0, 1, 0, 0, 1, 1, 1],\n",
    "                   [0, 0, 1, 0, 0, 1, 1, 1, 0, 1],\n",
    "                   [0, 0, 0, 1, 1, 0, 0, 0, 1, 1],\n",
    "                   [0, 0, 0, 0, 0, 1, 1, 1, 1, 1]])\n",
    "\n",
    "def zs_bk1(arr):#将z_score矩阵化为分类矩阵的函数\n",
    "    starttime = time.time()\n",
    "    \n",
    "    arrvec = []\n",
    "    for i in range(len(arr[0])):\n",
    "        d_len = len(arr[:, i])-len(np.where(np.isnan(arr[:, i]))[0])\n",
    "        remainder = d_len % 10\n",
    "        numorg = d_len // 10\n",
    "        condi = (modmat[:, remainder] + numorg).cumsum()\n",
    "        rankmat = pd.DataFrame(arr[:, i]).rank(ascending=False, \n",
    "                                               method='min').values\n",
    "        for j in range(10):\n",
    "            if j == 0:\n",
    "                rankmat[rankmat <= condi[j]] = j+1\n",
    "            else:\n",
    "                rankmat[(rankmat <= condi[j]) & (rankmat > condi[j-1])] = j+1\n",
    "        arrvec.append(rankmat)\n",
    "    arrmat = np.concatenate(arrvec,axis=1)\n",
    "    endtime= time.time()\n",
    "    t = endtime - starttime\n",
    "#     print('time_last: {} min'.format(t/60))\n",
    "    return arrmat\n",
    "\n",
    "def bk_rt1(bk_arr):#将分类矩阵化为按类收益的矩阵\n",
    "    starttime=time.time()\n",
    "    rtr_row = []\n",
    "    for i in range(len(bk_arr[0])-1):\n",
    "        rtr_col = []\n",
    "        for j in range(1, 11):\n",
    "            cod_ind = np.where(bk_arr[:, i] == j)[0]\n",
    "            bk_rtr_per = pd.DataFrame(stock_rt[cod_ind, i+1]).sum()[0]\n",
    "            nna = len(cod_ind)\n",
    "            bk_rtr_avg = bk_rtr_per/nna\n",
    "            rtr_col.append(bk_rtr_avg)\n",
    "        rtr_row.append(np.array(rtr_col))\n",
    "    bkrtmat = np.concatenate(rtr_row).reshape(len(bk_arr[0])-1, 10).T\n",
    "    endtime = time.time()\n",
    "#     print('timelast: {} min'.format((endtime-starttime)/60))\n",
    "    return bkrtmat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stock_rt_bk = zs_bk1(stock_rt)#把股票收益分类\n",
    "\n",
    "#把所有因子矩阵化为bk_rt矩阵\n",
    "for info in ml_names_all:\n",
    "    arr = globals()[info]\n",
    "    globals()[info+'_bk_rt_p'] = bk_rt1(zs_bk1(arr))\n",
    "\n",
    "#求所有因子的准确率的list，一个因子对应一个list，list里就是每天对应的准确率\n",
    "for info in ml_names_all:\n",
    "    globals()['accu_'+info] = []\n",
    "    ml_bk = zs_bk1(globals()[info])\n",
    "    for i in range(len(stock_rt[0])-1):\n",
    "        concatvec = np.concatenate([ml_bk[:,i], \n",
    "                                    stock_rt_bk[:,i+1]]).reshape(2,len(stock_rt)).T\n",
    "        concatvec = pd.DataFrame(concatvec).dropna().values\n",
    "        accu = metrics.accuracy_score(concatvec[:,0], concatvec[:,1])\n",
    "        globals()['accu_'+info].append(accu)\n",
    "\n",
    "#求所有因子的 与未来一天收益的 IC值list，一个因子对应一个list，list里就是每天对应的IC值\n",
    "for info in ml_names_all:\n",
    "    globals()['corr_'+info] = []\n",
    "    ml_zs = globals()[info]\n",
    "    for i in range(len(stock_rt[0])-1):\n",
    "        concatvec = np.concatenate([ml_zs[:,i], \n",
    "                                    stock_rt[:,i+1]]).reshape(2,len(stock_rt)).T\n",
    "        concatvec = pd.DataFrame(concatvec).dropna().values\n",
    "        corr = spearmanr(concatvec[:,0],concatvec[:,1]).correlation\n",
    "        globals()['corr_'+info].append(corr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def slide20(data):\n",
    "    return pd.DataFrame(data).rolling(window=20,center=False).mean().iloc[:,-1].values\n",
    "\n",
    "def cumsum_max(arr, fa_t, fa_len):\n",
    "# 根据上面的bk_rt求时间区间内的累积收益（做了滑动平均）的最大值。时间区间是根据fa_t, fa_len确定的\n",
    "# fa_t即之后机器学预测的某天的前一天的值，fa_len即取fa_t的前多少天用于计算累积收益、准确率和IC值\n",
    "    cum_arr = pd.DataFrame(arr[0, fa_t-fa_len+1:fa_t+1]).cumsum().values.T[0]#做多未做空\n",
    "    sld20_cum_arr = pd.DataFrame(slide20(list(cum_arr)))\n",
    "    arr_max = sld20_cum_arr.max()[0]  \n",
    "    return arr_max\n",
    "\n",
    "def accu_avg(accu_ml_l, fa_t, fa_len):#求相应时间区间内的准确率（做了滑动平均）的平均值\n",
    "    sld20_accu_ml_df = pd.DataFrame(slide20(accu_ml_l[fa_t-fa_len:fa_t]))\n",
    "    sld20_mean = sld20_accu_ml_df.mean()[0]    \n",
    "    return sld20_mean\n",
    "\n",
    "def corr_avg(corr_ml_l, fa_t, fa_len):#求相应时间区间内的IC值（做了滑动平均）的平均值\n",
    "    sld20_corr_ml_df = pd.DataFrame(slide20(corr_ml_l[fa_t-fa_len:fa_t]))###\n",
    "    sld20_mean = sld20_corr_ml_df.mean()[0]#[0]      \n",
    "    return sld20_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def three_condi_arr(fa_len, fa_t):\n",
    "#这个函数是求出相应时间区间内的三个arr（类似于list的形式），比如max_arr就是171个因子在样本区间内的累积收益滑动平均最大值\n",
    "    max_list = []\n",
    "    accu_describe_list = []\n",
    "    corr_describe_list = []\n",
    "    for info in ml_names_all:\n",
    "        arr = globals()[info+'_bk_rt_p']\n",
    "        max_list.append(cumsum_max(arr, fa_t, fa_len))\n",
    "        \n",
    "        accu_ml_l = globals()['accu_'+info]\n",
    "        accu_describe_list.append(accu_avg(accu_ml_l, fa_t, fa_len))\n",
    "        \n",
    "        corr_ml_l = globals()['corr_'+info]\n",
    "        corr_describe_list.append(corr_avg(corr_ml_l, fa_t, fa_len))\n",
    "        \n",
    "    max_arr = np.array(max_list)\n",
    "    accu_arr = np.array(accu_describe_list)\n",
    "    corr_arr = np.array(corr_describe_list)\n",
    "\n",
    "    return max_arr, accu_arr, corr_arr\n",
    "\n",
    "def factor_acr_t(intersc_k, max_arr, accu_arr, corr_arr):\n",
    "#将去掉重复因子名字的max_arr, accu_arr, corr_arr剩余因子相应的intersc_k即取排名前多少的股票做交集 \n",
    "    max_rank_arr = pd.DataFrame(max_arr).rank(ascending=False, \n",
    "                                              method='min').values.T[0]\n",
    "    accu_rank_arr = pd.DataFrame(accu_arr).rank(ascending=False, \n",
    "                                                method='min').values.T[0]\n",
    "    corr_rank_arr = pd.DataFrame(corr_arr).rank(ascending=False, \n",
    "                                                method='min').values.T[0]\n",
    "    fac_condi = np.concatenate([max_rank_arr, \n",
    "                                accu_rank_arr, \n",
    "                                corr_rank_arr]).reshape(3,len(ml_names_all)).T\n",
    "    \n",
    "    cumrt_idx = np.where(fac_condi[:,0] <= intersc_k)[0]\n",
    "    accu_idx = np.where(fac_condi[:,1] <= intersc_k)[0]\n",
    "    corr_idx = np.where(fac_condi[:,2] <= intersc_k)[0]\n",
    "    \n",
    "    condi_idx = list(set(cumrt_idx).intersection(set(accu_idx), set(corr_idx)))\n",
    "    condi_names = [ml_names_all[i] for i in condi_idx]\n",
    "    return sorted(condi_names)\n",
    "\n",
    "def mlnames_func(fa_len, fa_t, intersc_k):\n",
    "#这个函数就是把这个cell里前面几个函数逐一顺序拼起来\n",
    "    max_arr, accu_arr, corr_arr = three_condi_arr(fa_len, fa_t)\n",
    "    ml_names = factor_acr_t(intersc_k, max_arr, accu_arr, corr_arr)\n",
    "    return ml_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fa_len = 240\n",
    "intersc_k = 50#每个条件取前30求交集\n",
    "\n",
    "ml_names_listog = []\n",
    "stime = time.time()\n",
    "for fa_tf in range(fa_len, len(stock_rt[0])):#2595是20170901，2675是20171229\n",
    "    ml_names_t = mlnames_func(fa_len, fa_tf, intersc_k)\n",
    "    ml_names_listog.append(ml_names_t)\n",
    "etime = time.time()\n",
    "print('timelast {} min'.format((etime-stime)/60))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ml_names_list = [[] for i in range(fa_len)]+ml_names_listog#保证这个list跟全时期长度一样"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vad_index(t):#去除st,退市股票和无行业股票后剩下的股票的有效索引\n",
    "    st_indx = (np.where(valid_st_stock[:, t] == 1))[0]\n",
    "\n",
    "    ipo_date[ipo_date <= trd_dates[t]] = 1\n",
    "    ipo_date[ipo_date > trd_dates[t]] = 0\n",
    "    ipo_indx = np.where(ipo_date == 0)[0]\n",
    "\n",
    "    class_na_indx = np.where(np.isnan(gfn1_code_index[:, t]))[0]\n",
    "\n",
    "    stock_indx = np.arange(stock_rt.shape[0])\n",
    "\n",
    "    valid_indx = list(set(stock_indx).\n",
    "                      difference(set(ipo_indx).union(set(class_na_indx), \n",
    "                                                     set(st_indx))))\n",
    "\n",
    "    return valid_indx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fa_rank_rtbk(start_t, end_t):\n",
    "#进行排名\n",
    "#输入样本期起始天和要预测的那天，即样本期末尾那天的后一天，输出一个list，list里有相应样本期长度对应个数的XY数组，及要预测那天的有效股票索引\n",
    "    fa_total = []\n",
    "    for t in range(start_t, end_t+1):\n",
    "        fa_cmb = []\n",
    "        vad_id = vad_index(t)\n",
    "        for info in ml_names_list[end_t-1]:      \n",
    "            fa_vec = globals()[info][vad_id, t]\n",
    "            fa_vec_df = pd.DataFrame(fa_vec).rank(ascending=False, method='dense')\n",
    "            fa_vec_arr = fa_vec_df.values\n",
    "            nna_len = len(fa_vec_df.dropna())\n",
    "            fa_vec_rank_arr = fa_vec_arr / len(fa_vec_df.dropna())\n",
    "            fa_cmb.append(fa_vec_rank_arr.T[0])          \n",
    "        fa_cmb.append(stock_rt[vad_id, t+1])\n",
    "        \n",
    "        fa_cmb_cc = np.concatenate(fa_cmb).reshape((len(ml_names_list[end_t-1])+1), len(vad_id)).T\n",
    "        if t == end_t:\n",
    "            na_idx = np.unique(np.where(np.isnan(fa_cmb_cc))[0])\n",
    "            nna_idx = sorted(list(set(np.arange(len(vad_id))).difference(set(na_idx))))\n",
    "            nna_idx_t = [vad_id[k] for k in nna_idx]\n",
    "        fa_delna = pd.DataFrame(fa_cmb_cc).dropna().values\n",
    "        fa_total.append(fa_delna)\n",
    "\n",
    "    return fa_total, nna_idx_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fa_rtbk(start_t, end_t):\n",
    "#未排序，上面是排序的\n",
    "#输入样本期起始天和要预测的那天，即样本期末尾那天的后一天，输出一个list，list里有相应样本期长度对应个数的XY数组，及要预测那天的有效股票索引\n",
    "    fa_total = []\n",
    "\n",
    "    for t in range(start_t, end_t+1):\n",
    "        fa_cmb = []\n",
    "        vad_id = vad_index(t)\n",
    "        for info in ml_names_list[end_t-1]:            \n",
    "            fa_vec = globals()[info][vad_id, t]\n",
    "            fa_vec_df = pd.DataFrame(fa_vec)\n",
    "            fa_vec_arr = fa_vec_df.values\n",
    "            nna_len = len(fa_vec_df.dropna())\n",
    "            fa_vec_rank_arr = fa_vec_arr\n",
    "            fa_cmb.append(fa_vec_rank_arr.T[0])          \n",
    "        fa_cmb.append(stock_rt[vad_id, t+1])\n",
    "        \n",
    "        fa_cmb_cc = np.concatenate(fa_cmb).reshape((len(ml_names_list[end_t-1])+1), \n",
    "                                                   len(vad_id)).T\n",
    "        if t == end_t:\n",
    "            na_idx = np.unique(np.where(np.isnan(fa_cmb_cc))[0])\n",
    "            nna_idx = sorted(list(set(np.arange(len(vad_id))).difference(set(na_idx))))\n",
    "            nna_idx_t = [vad_id[k] for k in nna_idx]\n",
    "        fa_delna = pd.DataFrame(fa_cmb_cc).dropna().values\n",
    "        fa_total.append(fa_delna)\n",
    "\n",
    "    return fa_total, nna_idx_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(factor_cat[60])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rfc(factor_sp, factor_test, n_est):# Random Tree Forest Model\n",
    "    rf = RandomForestClassifier(n_estimators=n_est, \n",
    "                                random_state=1, \n",
    "                                n_jobs=4)\n",
    "    rf.fit(factor_sp[:, :-1], factor_sp[:, -1])\n",
    "    y_test_pred = rf.predict_proba(factor_test[:, :-1])\n",
    "    class_ord = rf.classes_\n",
    "    fa_importance = rf.feature_importances_\n",
    "    return y_test_pred, class_ord, fa_importance\n",
    "\n",
    "\n",
    "def sp_testmat(factor_cat_p):\n",
    "    factor_sp = np.concatenate(factor_cat_p[: -1])\n",
    "    factor_cat_sp = factor_cat[-1]\n",
    "    return factor_sp, factor_cat_sp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bys_b(factor_sp, factor_test, n_est):# Naive Bayes Model-Bernoulli\n",
    "    by = BernoulliNB(alpha=1.0, binarize=0.0, \n",
    "                     class_prior=None, fit_prior=True)\n",
    "    by.fit(factor_sp[:, :-1], factor_sp[:, -1])\n",
    "    y_test_pred = by.predict_proba(factor_test[:, :-1])\n",
    "    class_ord = by.classes_\n",
    "    return y_test_pred, class_ord\n",
    "\n",
    "\n",
    "def bys_m(factor_sp, factor_test, n_est):# Naive Bayes Model-Polynomial\n",
    "    by = MultinomialNB(alpha=1.0, class_prior=None, \n",
    "                       fit_prior=True)\n",
    "    by.fit(factor_sp[:, :-1], factor_sp[:, -1])\n",
    "    y_test_pred = by.predict_proba(factor_test[:, :-1])\n",
    "    class_ord = by.classes_\n",
    "    return y_test_pred, class_ord\n",
    "\n",
    "\n",
    "def bys_g(factor_sp, factor_test, n_est):# Naive Bayes Model-Gaussian\n",
    "    by = GaussianNB(priors=None)\n",
    "    by.fit(factor_sp[:, :-1], factor_sp[:, -1])\n",
    "    y_test_pred = by.predict_proba(factor_test[:, :-1])\n",
    "    class_ord = by.classes_\n",
    "    return y_test_pred, class_ord"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lgbm(factor_sp, factor_test, n_est):#LightGBM model\n",
    "    lgb =  LGBMClassifier(boosting_type='gbdt', objective= 'binary', \n",
    "                          n_jobs= 4, n_estimators=100, num_leaves= 31, \n",
    "                          max_depth= -1, learning_rate=0.1)\n",
    "    lgb.fit(factor_sp[:, :-1], factor_sp[:, -1])\n",
    "    y_test_pred = lgb.predict_proba(factor_test[:, :-1])\n",
    "    class_ord = lgb.classes_\n",
    "    fa_importance = lgb.feature_importances_\n",
    "    return y_test_pred, class_ord, fa_importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def l_rg(factor_sp, factor_test, n_est):# Logistic Model\n",
    "    lr =  LogisticRegression(solver='saga', n_jobs= 4)\n",
    "    lr.fit(factor_sp[:, :-1], factor_sp[:, -1])\n",
    "    y_test_pred = lr.predict_proba(factor_test[:, :-1])\n",
    "    class_ord = []\n",
    "    return y_test_pred, class_ord"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ratio1(factor_cat, r1):\n",
    "    factor_cat_copy = copy.deepcopy(factor_cat)\n",
    "    factor_cat_p = []\n",
    "    for i in range(len(factor_cat_copy)):\n",
    "        factor_cat_cp = factor_cat_copy[i]\n",
    "        sort_idx = np.argsort(-factor_cat_cp[:, -1])\n",
    "        top_idx = sort_idx[:int(r1 * len(sort_idx))]\n",
    "        bottm_idx = sort_idx[-int(r1 * len(sort_idx)):]\n",
    "        factor_cat_cp[top_idx, -1] = 0\n",
    "        factor_cat_cp[bottm_idx, -1] = 1\n",
    "        factor_cat_p.append(factor_cat_cp[list(top_idx)+list(bottm_idx), :])\n",
    "    return factor_cat_p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ratio2(y_pred_list, r2):\n",
    "# 构建股票策略选取的股票比例，y_pred_list即机器学习输出的预测结果，也就是概率矩阵\n",
    "    y_pred_list_copy = copy.deepcopy(y_pred_list)\n",
    "    mean_total = []\n",
    "    corr_list = []\n",
    "    top_idx_list = []\n",
    "    bottm_idx_list = []\n",
    "    for i in range(sld_len):\n",
    "        prob_len = len(y_pred_list_copy[i][:, 0])\n",
    "        factor_cat_sp2 = factor_test_list[i]\n",
    "        prob_rt = np.concatenate([y_pred_list_copy[i][:,0], \n",
    "                                  factor_cat_sp2[:,-1]]).reshape(2, prob_len).T\n",
    "        \n",
    "        sort_idx = np.argsort(-prob_rt[:, 0])\n",
    "        top_idx = sort_idx[:int(r2 * len(sort_idx))]\n",
    "        bottm_idx = sort_idx[-int(r2 * len(sort_idx)):]\n",
    "        top_idx_list.append(list(top_idx))\n",
    "        bottm_idx_list.append(list(bottm_idx))\n",
    "\n",
    "        top_mean = pd.DataFrame(prob_rt[top_idx, 1]).mean()\n",
    "        bottm_mean = pd.DataFrame(prob_rt[bottm_idx, 1]).mean()\n",
    "        mean_total.append(top_mean)\n",
    "        mean_total.append(bottm_mean)\n",
    "        \n",
    "        concatvec = pd.DataFrame(prob_rt).dropna().values\n",
    "        corr = spearmanr(concatvec[:,0],concatvec[:,1]).correlation\n",
    "        corr_list.append(corr)\n",
    "    \n",
    "    mean_total_arr = np.array(mean_total).reshape(sld_len,2).T\n",
    "    return mean_total_arr, corr_list, top_idx_list, bottm_idx_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def postion(nna_idx_list, top_idx_list, bottm_idx_list):\n",
    "# 输出多空组合持仓数组及相应权重（等权的）\n",
    "    posmat_list = []\n",
    "    for i in range(sld_len):\n",
    "        nna_idxp = nna_idx_list[i]\n",
    "        top_idxp = top_idx_list[i]\n",
    "        bottm_idxp = bottm_idx_list[i]\n",
    "        top_len = len(top_idxp)\n",
    "        bottm_len = len(bottm_idxp)\n",
    "        posmat = np.zeros((len(stock_rt), 1))\n",
    "        \n",
    "        top_idx_org = [nna_idxp[k] for k in top_idxp]\n",
    "        bottm_idx_org = [nna_idxp[k] for k in bottm_idxp]\n",
    "        \n",
    "        posmat[top_idx_org] = 1/top_len\n",
    "        posmat[bottm_idx_org] = -1/bottm_len\n",
    "        posmat_list.append(posmat)\n",
    "    \n",
    "    pos_arr = np.concatenate(posmat_list, axis=1)\n",
    "#    pos_df = pd.DataFrame(posmat_arr, columns=[trd_dates[i][0] for i in range(end_t+1, end_t+1+sld_len)])\n",
    "    return pos_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def postion_buy(nna_idx_list, top_idx_list):\n",
    "# 输出做多组合的持仓股票，标记为1，无权重\n",
    "    posmat_list = []\n",
    "    for i in range(sld_len):\n",
    "        nna_idxp = nna_idx_list[i]\n",
    "        top_idxp = top_idx_list[i]\n",
    "#         bottm_idxp = bottm_idx_list[i]\n",
    "        top_len = len(top_idxp)\n",
    "#         bottm_len = len(bottm_idxp)\n",
    "        posmat = np.zeros((len(stock_rt), 1))\n",
    "        \n",
    "        top_idx_org = [nna_idxp[k] for k in top_idxp]\n",
    "#         bottm_idx_org = [nna_idxp[k] for k in bottm_idxp]\n",
    "        \n",
    "        posmat[top_idx_org] = 1\n",
    "#         posmat[bottm_idx_org] = -1/bottm_len\n",
    "        posmat_list.append(posmat)\n",
    "    \n",
    "    pos_arr = np.concatenate(posmat_list, axis=1)\n",
    "#    pos_df = pd.DataFrame(posmat_arr, columns=[trd_dates[i][0] for i in range(end_t+1, end_t+1+sld_len)])\n",
    "    return pos_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fa_t = 240\n",
    "fit_len = 40\n",
    "sld_len = 2750-1-fit_len-fa_t#未取倒数第二天，即未预测最后一天收益\n",
    "end_t = fa_t+fit_len-1# 60+fit_len-1\n",
    "n_est = 30\n",
    "r1 = 0.1\n",
    "\n",
    "\n",
    "y_pred_list = []\n",
    "class_ord_list = []\n",
    "importance_list = []\n",
    "nna_idx_list = []\n",
    "factor_test_list = []\n",
    "\n",
    "starttime = time.time()\n",
    "for i in range(sld_len):\n",
    "    endtime1 = time.time()\n",
    "    sap_t1 = end_t-fit_len+1+i\n",
    "    sap_t2 = end_t+1+i\n",
    "    factor_cat, nna_idx, mlname = fa_rank_rtbk(sap_t1, sap_t2)\n",
    "    nna_idx_list.append(nna_idx)\n",
    "    \n",
    "    factor_cat_p = ratio1(factor_cat, r1)\n",
    "    factor_sp, factor_test = sp_testmat(factor_cat_p)\n",
    "    factor_test_list.append(factor_test)\n",
    "    y_test_pred, class_order, fa_importance = rfc(factor_sp, \n",
    "                                                  factor_test, n_est)\n",
    "\n",
    "    y_pred_list.append(y_test_pred)\n",
    "    importance_list.append(fa_importance)\n",
    "    class_ord_list.append(class_order)\n",
    "    endtime2 = time.time()\n",
    "#    print('processed {} day, timelast: {} min'.format(i, ((endtime2-endtime1)/60)))\n",
    "endtime3 = time.time()\n",
    "print('timelast: {} min'.format((endtime3-starttime)/60))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fa_t = 240\n",
    "fit_len = 40\n",
    "sld_len = 2750-1-fit_len-fa_t#未取倒数第二天，即未预测最后一天收益\n",
    "end_t = fa_t+fit_len-1#  60+fit_len-1\n",
    "n_est = 30\n",
    "r1 = 0.15\n",
    "\n",
    "\n",
    "y_pred_list = []\n",
    "class_ord_list = []\n",
    "importance_list = []\n",
    "nna_idx_list = []\n",
    "factor_test_list = []\n",
    "\n",
    "\n",
    "starttime = time.time()\n",
    "for i in range(sld_len):\n",
    "    endtime1 = time.time()\n",
    "    sap_t1 = end_t-fit_len+1+i\n",
    "    sap_t2 = end_t+1+i\n",
    "    factor_cat, nna_idx = fa_rank_rtbk(sap_t1, sap_t2)\n",
    "\n",
    "    nna_idx_list.append(nna_idx)\n",
    "    \n",
    "    factor_cat_p = ratio1(factor_cat, r1)\n",
    "    factor_sp, factor_test = sp_testmat(factor_cat_p)\n",
    "    factor_test_list.append(factor_test)\n",
    "    y_test_pred, class_order, fa_importance = lgbm(factor_sp, factor_test, n_est)\n",
    "\n",
    "    y_pred_list.append(y_test_pred)\n",
    "    importance_list.append(fa_importance)\n",
    "    class_ord_list.append(class_order)\n",
    "    endtime2 = time.time()\n",
    "#    print('processed {} day, timelast: {} min'.format(i, ((endtime2-endtime1)/60)))\n",
    "endtime3 = time.time()\n",
    "print('timelast: {} min'.format((endtime3-starttime)/60))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r2 = 0.01\n",
    "mean_total_arr, corr_list, top_idx_list, bottm_idx_list = ratio2(y_pred_list, r2)\n",
    "\n",
    "pos_arr = postion(nna_idx_list, top_idx_list, bottm_idx_list)\n",
    "pos_df = pd.DataFrame(pos_arr, columns=[trd_dates[i][0] for i in range(end_t+1, end_t+1+sld_len)])\n",
    "pos_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_df.to_csv('pos_df.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_cumsum = pd.DataFrame(mean_total_arr).T.cumsum().T\n",
    "\n",
    "line = Line('cum_rtr')\n",
    "arr = mean_cumsum.values\n",
    "attr = [trd_dates[i] for i in range(end_t+1, end_t+sld_len+1)]\n",
    "line.add('buy1set', attr, slide20(list(arr[0])), mark_line=['max'])\n",
    "line.add('buy1sell1set', attr,  slide20(list(arr[0]-arr[1])), mark_line=['max'])\n",
    "line.add('buy10set', attr, slide20(list(arr[1])), mark_line=['min'])\n",
    "line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_ml = corr_list\n",
    "attr = [trd_dates[i] for i in range(end_t+1, end_t+sld_len+1)]\n",
    "line = Line('IC')\n",
    "line.add('ic', attr, slide20(corr_ml),mark_line=['average', 'max', 'min'])\n",
    "line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 全时期选用的相应因子及其权重，并画出热力图\n",
    "# select factors and corresponding weighht based on the whole time period and draw heatmap\n",
    "factor_weight = pd.DataFrame(np.zeros((len(ml_names_all), stock_rt.shape[1])), \n",
    "                             index=ml_names_all)\n",
    "for i in tnrange(sld_len):\n",
    "    sap_t2 = end_t+1+i\n",
    "    for j, info in enumerate(ml_names_list[sap_t2-1]):\n",
    "        factor_weight.loc[info, sap_t2-1] = importance_list[i][j] / importance_list[i].sum()\n",
    "        \n",
    "\n",
    "ml_union_list = []\n",
    "for i in tnrange(sld_len):\n",
    "    starttime = time.time()\n",
    "    sap_t2 = end_t+1+i\n",
    "    ml_union_list.append(ml_names_list[sap_t2-1])\n",
    "ml_union = sorted(list(np.unique(np.concatenate(ml_union_list))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "factor_weight_s = 1000*factor_weight.loc[ml_union, end_t: end_t+sld_len-1]\n",
    "\n",
    "x_axis = [trd_dates[end_t+i] for i in range(sld_len)]\n",
    "y_axis = ml_union\n",
    "data = [[i, j, factor_weight_s.iloc[j, i]] \n",
    "        for i in range(sld_len) \n",
    "        for j in range(factor_weight_s.shape[0])]\n",
    "heatmap = HeatMap(width=1000, height=1600)\n",
    "heatmap.add('factor_weight_heatmap', x_axis, y_axis, \n",
    "            data, is_visualmap=True,\n",
    "            visual_text_color='#000', visual_orient='horizontal',\n",
    "            calendar_cell_size=[0.5,0.5],\n",
    "            visual_range_color=['#FFFFFF', '#FFE4B5','#FFA500'])\n",
    "#            is_datazoom_show=True, datazoom_orient='vertical')\n",
    "heatmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 做多策略中行业股票数量的河流图\n",
    "# stream graph to illustrate different industries of the stocks selcted in long strategy\n",
    "posmat_buy = postion_buy(nna_idx_list, top_idx_list)\n",
    "gfn1_code_s = pd.DataFrame(gfn1_code_index[:, end_t+1: end_t+1+sld_len])\n",
    "gfn1_pos_buy = gfn1_code_s.values * posmat_buy\n",
    "gfn1_code_num = np.zeros((11, gfn1_pos_buy.shape[1]))\n",
    "gfn1_sum = gfn1_code_s.sum()\n",
    "for i in range(gfn1_pos_buy.shape[1]):\n",
    "    for j in range(11):\n",
    "        gfn1_code_num[j, i] = len(np.where(gfn1_pos_buy[:,i] == j+1)[0]) / gfn1_sum[i]\n",
    "        \n",
    "gfn1_name = ['金融','房地产','周期行业','信息技术','可选消费','必选消费','医药生物','交通运输','公用事业','材料','工业']\n",
    "data = []\n",
    "for i in range(gfn1_code_num.shape[1]):\n",
    "    for j in range(gfn1_code_num.shape[0]):\n",
    "        data_p = []\n",
    "        date = trd_dates[end_t+1+i][0]\n",
    "        data_p.append(str(date)[:4]+'/'+str(date)[4:6]+'/'+str(date)[6:8])\n",
    "        data_p.append(gfn1_code_num[j, i])\n",
    "        data_p.append(gfn1_name[j])\n",
    "        data.append(data_p)\n",
    "        \n",
    "data_fill = [[] for i in range((end_t+1)*11)] + data + [[] for i in range(2*11)]\n",
    "year_end = []\n",
    "for i in range(2008,2020):\n",
    "    trd_t = np.where((trd_dates <= i*10000))[0][-1]\n",
    "    year_end.append(trd_t)\n",
    "year_end_f = [-1/11] + year_end\n",
    "\n",
    "tr = ThemeRiver(width=800)\n",
    "tr.add([gfn1_name[i] for i in range(gfn1_code_num.shape[0])], data, is_label_show=True,is_datazoom_show=True, yaxis_margin =20,yaxis_name_gap=100)\n",
    "tr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k in range(len(year_end)):\n",
    "    globals()['tr'+str(k)] = ThemeRiver(str(0)+str(k+7))\n",
    "    globals()['tr'+str(k)].add([gfn1_name[i] for i in range(gfn1_code_num.shape[0])], \n",
    "                           data_fill[int(year_end_f[k]*11+1):year_end_f[k+1]*11+1], \n",
    "                           is_label_show=True, is_datazoom_show=True, datazoom_range=[0,25])\n",
    "\n",
    "page = Page()\n",
    "for k in range(len(year_end)):#len(year_end)\n",
    "    page.add(globals()['tr'+str(k)])\n",
    "page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid = Grid()\n",
    "\n",
    "attr = ['LGBM', 'RF', 'NB', 'LR']\n",
    "v1 = [26.18, 22.84, 15.91, 20.33]\n",
    "v3 = [10.64, 8.84, 9.05, 7.33]\n",
    "v4 = [68, 106, 90, 100]\n",
    "\n",
    "bar1 = Bar(width=1200, height=600)\n",
    "bar1.add(\"多空组合收益\", attr, v1, yaxis_name=\"cum_rt\")# long+short return\n",
    "bar2 = Bar()\n",
    "bar2.add(\"做多收益\", attr, v3)# long return\n",
    "bar3 = Bar()\n",
    "bar3.add(\"时间\", attr, v4, yaxis_name=\"time/min\")\n",
    "\n",
    "\n",
    "\n",
    "overlap = Overlap()\n",
    "overlap.add(bar1)\n",
    "overlap.add(bar2, is_add_yaxis=True, yaxis_index=0)\n",
    "overlap.add(bar3, is_add_yaxis=True, yaxis_index=1)\n",
    "\n",
    "grid.add(overlap, grid_right=\"10%\")\n",
    "grid"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
